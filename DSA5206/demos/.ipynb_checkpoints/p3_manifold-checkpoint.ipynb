{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold learning\n",
    "\n",
    "In this notebook, we implement various manifold learning techniques on a simple dataset.\n",
    "\n",
    "Our goal is to see\n",
    "- how to use existing libraries to perform manifold learning algorithms\n",
    "- how the different algorithms produce different results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "np.random.seed(5206)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The swiss roll dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The swiss roll dataset is a popular toy problem in manifold learning.\n",
    "\n",
    "![Swiss Roll](https://upload.wikimedia.org/wikipedia/commons/d/da/Sri_Lankan_Swiss_roll.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate data resembling a swiss roll via a parametric equation\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    x &= \\frac{1}{6} (\\phi + \\text{noise}) \\sin(\\phi) \\\\\n",
    "    y &= \\frac{1}{6} (\\phi + \\text{noise}) \\cos(\\phi) \\\\\n",
    "    z &\\sim U[0, L_z] \\text{ and } \\phi \\sim U[0, L_\\phi]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "length_phi = 15   # Length of swiss roll in angular direction\n",
    "length_Z = 15     # Length of swiss roll in z direction\n",
    "sigma = 0.1       # Noise strength\n",
    "m = 8192          # Number of samples\n",
    "\n",
    "# Create dataset\n",
    "phi = length_phi * np.random.rand(m)\n",
    "xi = np.random.rand(m)\n",
    "z = length_Z * np.random.rand(m)\n",
    "x = 1./6 * (phi + sigma * xi) * np.sin(phi)\n",
    "y = 1./6 * (phi + sigma * xi) * np.cos(phi)\n",
    "\n",
    "X = np.array([x, y, z]).transpose()\n",
    "\n",
    "color = phi  # color the data according to the angular direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.view_init(elev=45, azim=30)\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color)\n",
    "ax.set_xlabel(r'$X$')\n",
    "ax.set_ylabel(r'$Y$')\n",
    "ax.set_zlabel(r'$Z$')\n",
    "ax.set_title('Swiss roll dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the following:\n",
    "- The dataset is in 3D\n",
    "- The manifold (the swiss roll) is a 2D surface embedded in 3D\n",
    "- The colour shown is a \"coordinate\" on this 2D surface\n",
    "\n",
    "The goal of manifold learning is to find an embedding so that the \"coordinate\" color can be correlated to the embedded space.\n",
    "\n",
    "Thus, a good \"principal component\" will be on that varies continuously with the color gradient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)  # we shall just plot the top 2 components\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the scores for the top two components..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=color, legend=False)\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.title('Scatter plot of PCA component scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that when projected to this 2D space, the manifold structure is *not* sufficiently captured.\n",
    "\n",
    "In particular, points very far on the manifold have very close projections!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we try kernel PCA with the RBF kernel to see if we can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca = KernelPCA(n_components=2, kernel='rbf', gamma=1.0)\n",
    "X_kpca = kpca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the RBF kernel with parameter `gamma` ($\\gamma$), i.e.\n",
    "$$\n",
    "    k(x,x') = \\exp\n",
    "    \\left[\n",
    "        - \\frac{|x-x'|^2}{2\\gamma^2}\n",
    "    \\right]\n",
    "$$\n",
    "In practice, $\\gamma$ has to be tuned for best results (say with cross validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_kpca[:, 0], y=X_kpca[:, 1], hue=color, legend=False)\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.title('Scatter plot of kernel PCA component scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that this is an improvement to the linear PCA, but still still not satisfactory!\n",
    "\n",
    "Of course, the choice of kernels are very important\n",
    "- can you test some different kernels?\n",
    "- can you think of constructing a kernel that works well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally linear embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us apply locally linear embedding.\n",
    "\n",
    "We can see that for this problem, if we pick a moderate number of nearest neighbours, the linear structure can capture much of the manifold's geometry!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lle = LocallyLinearEmbedding(n_neighbors=10, n_components=2, method='standard')\n",
    "X_lle = lle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we pick 10 nearest neighbours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_lle[:, 0], y=X_lle[:, 1], hue=color, legend=False)\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.title('Scatter plot of LLE components')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that LLE is quite effective for this problem, and the two components map out much of the manifold.\n",
    "\n",
    "However, do note that the choice of the number of nearest neighbours is important!\n",
    "- Can you see why?\n",
    "- Test it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` currently does not support diffusion maps, but we can use another implementation `pyDiffMap`.\n",
    "\n",
    "You can read about it [here](https://pydiffmap.readthedocs.io/en/master/readme.html).\n",
    "\n",
    "You can install this package by\n",
    "```shell\n",
    "pip install pyDiffMap\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydiffmap import diffusion_map as dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmap = dm.DiffusionMap.from_sklearn(n_evecs=2, k=200, epsilon='bgh', alpha=0.0)\n",
    "X_dmap = dmap.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters are worth noting\n",
    "- `k` is the number of nearest neighbours to construct the kernel (and hence the transition matrix). This is supposed to be num_data-points, but we can save computation by having fewer points\n",
    "- `alpha` is an optional scaling parameter. Setting to 0 reduces to the simplest construction of the transition matrix, but in practice this should be tuned. Try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_dmap[:, 0], y=X_dmap[:, 1], hue=color, legend=False)\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.title('Scatter plot of diffusion map components')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the manifold structure is well preserved - with component 1 similar to the angular coordinate and component 2 the perpendicular direction of the swiss roll."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we played around with various manifold learning (non-linear dimensionality reduction) techniques, and compared them on the simple swiss roll dataset.\n",
    "\n",
    "Exercise\n",
    "- Play around with the various hyper-parameters (read the docs!), if you use these techniques for your project, you will need to tune these parameters.\n",
    "- Can you try these methods on a more realistic dataset, say MNIST, FashionMNIST or something even more complicated? What are some computational issues one may encounter?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsa5206_202324",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
