{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic mode decomposition\n",
    "\n",
    "In this notebook, we will perform extended dynamic mode decomposition (EDMD) on a simple 2D non-linear dynamical system.\n",
    "Our goal is to see\n",
    "- how to make predictions using EDMD\n",
    "- how to visualise the predictions, eigenfunctions, eigenvalues etc\n",
    "\n",
    "We will implement things from scratching using only standard libraries like `numpy` and `sklearn`.\n",
    "We will then mention some ready-made packages under active development for Koopman/DMD based analysis, where you can learn much more about the subject.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "np.random.seed(5206)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Van der Pol oscillator\n",
    "\n",
    "We will investigate a discretised version of the [Van der Pol oscillator](https://en.wikipedia.org/wiki/Van_der_Pol_oscillator), which is a simple non-linear dynamical system that exhibit non-trivial limit cycles.\n",
    "These oscillators were used to model a variety of phenomena, from the actions of neurons to the movement of tectonic plates.\n",
    "\n",
    "The Van der Pol oscillator is a second-order non-linear ODE, which when discretised gives a system of first-order, non-linear difference equations\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        x(t+1) &= x - \\delta t \\, y(t) \\\\\n",
    "        y(t+1) &= y + \\delta t \\, (x(t) - y(t) + x(t)^2 y(t))\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "We first write a simulator for this equation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vdp_simulator(initial_state: np.ndarray, dt: float, num_steps: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Simulates the Van der Pol oscillator.\n",
    "\n",
    "    Args:\n",
    "        initial_state (ndarray): The initial state of the oscillator.\n",
    "        dt (float): The time step size.\n",
    "        num_steps (int): The number of simulation steps.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The time points and the states of the oscillator.\n",
    "    \"\"\"\n",
    "\n",
    "    def vdp(x: float, y: float) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Van der Pol oscillator function.\n",
    "\n",
    "        Args:\n",
    "            x (float): The current value of x.\n",
    "            y (float): The current value of y.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The next values of x and y.\n",
    "        \"\"\"\n",
    "        # Calculate the next values of x and y\n",
    "        x_next = x - dt * y\n",
    "        y_next = y + dt * (x - y + x**2 * y)\n",
    "\n",
    "        return x_next, y_next\n",
    "\n",
    "    times: List[float] = [0]\n",
    "    states: List[np.ndarray] = [initial_state]\n",
    "    state: np.ndarray = initial_state\n",
    "    for n in range(num_steps):\n",
    "        times.append(times[-1] + dt)\n",
    "        x, y = state[:, 0], state[:, 1]\n",
    "        x_next, y_next = vdp(x, y)\n",
    "        state = np.column_stack([x_next, y_next])\n",
    "        states.append(state)\n",
    "\n",
    "    return np.array(times), np.array(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simulate the VdP dynamics for 4 time steps, starting from 512 different initial conditions picked uniformly at random from the square $[-2,2]$. We use the step-size $\\delta t = 0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_traj = 512\n",
    "num_steps = 4\n",
    "dt = 0.1\n",
    "initial_state = np.random.uniform(-2, 2, size=(num_traj, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times, states = vdp_simulator(initial_state, dt, num_steps)  # Simulate the Van der Pol oscillator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at some example trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots = 32\n",
    "for i in range(num_plots):\n",
    "    plt.plot(states[:, i, 0], states[:, i, 1], 'o-')\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the system undergoes transient oscillations before decaying to 0, in a rather irregular orbit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDMD on the Van der Pol system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a dictionary\n",
    "\n",
    "We shall choose a commonly used dictionary consisting of radial basis functions. We have seen similar basis functions in previous demos.\n",
    "To recall, a (Gaussian) radial basis function is defined as a function from $\\R^n \\to \\R$\n",
    "$$\n",
    "    \\phi_{c,s}(x) = \\exp\n",
    "    \\left(\n",
    "        - | x - c |^2 / (2 * s^2)\n",
    "    \\right)\n",
    "$$\n",
    "where $c \\in \\R^n$ is the center, and $s \\in \\R$ is the standard deviation. We usually randomly sample $c,s$ in some range, so that this gives a \"basis\" set that is randomly generated.\n",
    "\n",
    "In the following, we write a simple dictionary class that does two things\n",
    "- Given some state, compute the observables spanned by the dictionary. We will use state-augmented dictionaries, where the first two dimensions of the dictionary are just the state observation functions $(x, y) \\mapsto x$ and $(x, y) \\mapsto y$ respectively.\n",
    "- Given the observable, reconstruct the state. Since we have included the state in the observable set, we can accomplish this by simply reading off the first two dictionary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "class RBFDictionary:\n",
    "    def __init__(self, centers: np.ndarray, stds: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the RBFDictionary.\n",
    "\n",
    "        Args:\n",
    "            centers (ndarray): The centers of the radial basis functions.\n",
    "            stds (ndarray): The standard deviations of the radial basis functions.\n",
    "        \"\"\"\n",
    "        self.feature_dim, self.state_dim = centers.shape\n",
    "        self.centers = centers  # (feature_dim, state_dim)\n",
    "        self.stds = stds  # (feature_dim)\n",
    "\n",
    "    def transform(self, states: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transforms the states into observables using the RBFDictionary.\n",
    "\n",
    "        Args:\n",
    "            states (ndarray): The states to be transformed.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: The transformed observables.\n",
    "        \"\"\"\n",
    "        distances = np.linalg.norm(\n",
    "            states[:, None, :] - self.centers[None, :, :],\n",
    "            axis=-1,\n",
    "        )  # (num_samples, feature_dim)\n",
    "        rbf_features =  np.exp(-distances**2 / (2 * self.stds[None, :]**2))\n",
    "        state_features = states\n",
    "        return np.concatenate([state_features, rbf_features], axis=-1)\n",
    "\n",
    "    def reconstruct(self, observables: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Reconstructs the states from the observables.\n",
    "\n",
    "        Args:\n",
    "            observables (ndarray): The observables to be reconstructed.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: The reconstructed states.\n",
    "        \"\"\"\n",
    "        return observables[:, :self.state_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now use the rbf dictionary to extract features from the state trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 2\n",
    "feature_dim = 64  # number of RBF functions\n",
    "centers = np.random.uniform(-2, 2, size=(feature_dim, state_dim))\n",
    "stds = np.random.uniform(0.5, 1.5, size=(feature_dim,))\n",
    "rbf = RBFDictionary(centers=centers, stds=stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximating the Koopman operator under the approximate invariant subspace spanned by the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first split the trajectories `state` with dimensions [time, sample, state_dimension] into inputs and outputs offset by 1 time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = states[:-1].reshape(-1, state_dim), states[1:].reshape(-1, state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_X, phi_Y = rbf.transform(X), rbf.transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the DMD can now be performed on this system by minimising\n",
    "$$\n",
    "    K = \\argmin_{K'} \\| \\phi(Y) - \\phi(X) {K'}^\\top \\|_F^2\n",
    "$$\n",
    "which can be solved by linear regression. Note that the linear regression routine in `sklearn` will automatically give $K^\\top$, so we need to take some care in transforming the learned $K$'s accordingly.\n",
    "We will use ridge regression to improve stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "reg = Ridge(alpha=1e-5, fit_intercept=False)\n",
    "reg.fit(phi_X, phi_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we check the mean squared error to make sure that this fitting is done well enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(phi_Y, reg.predict(phi_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can extract the finite-dimensional approximation of the Koopman operator as $K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us attempt to use the data-driven Koopman operator to make predictions on the state trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pred = 200  # number of test time steps\n",
    "initial_test = np.array([[0.5, 1.5]])  # test initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, true_trajectory = vdp_simulator(initial_state=initial_test, dt=dt, num_steps=num_pred)  # simulate the true trajectory for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we write a routine to use the Koopman operator to predict the states as a function of time.\n",
    "\n",
    "Note that the right way to think about this prediction is that, we are not predicting the states per se, but rather predicting the value of some observables that are in the span of our dictionary!\n",
    "\n",
    "The steps are as follows:\n",
    "1. We know that $\\phi(t) = K^t \\phi(0)$, so we can compute it via repeated multiplication. Note the transpose since we are dealing with row vectors in the implementation.\n",
    "2. We can extract the states by simply calling the `reconstruct` routine in our dictionary. This will give the prediction.\n",
    "3. To improve stability, we add a projection step, that is, instead of computing $\\phi(t) = K^t \\phi(0)$ in one go, at every step $\\phi(t+1) = K \\phi(t)$, we transform $\\phi(t+1)$ to $\\text{state}(t+1)$, then recompute the observable via the `transform` method. This is to ensure that the observables are always on the right \"manifold\" as it evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(initial_state: np.ndarray, K_matrix: np.ndarray, dictionary: RBFDictionary, num_pred: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predicts the states using the data-driven Koopman operator.\n",
    "\n",
    "    Args:\n",
    "        initial_state (ndarray): The initial state.\n",
    "        K_matrix (ndarray): The Koopman operator matrix.\n",
    "        dictionary (RBFDictionary): The dictionary used for feature transformation.\n",
    "        num_pred (int): The number of prediction steps.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: The predicted states.\n",
    "    \"\"\"\n",
    "    state = initial_state\n",
    "    predictions: List[np.ndarray] = [state]\n",
    "    phi = dictionary.transform(state)\n",
    "    for n in range(num_pred):\n",
    "        phi = phi @ K_matrix.T  # evolution step for observables\n",
    "        u = dictionary.reconstruct(phi)  # compute states\n",
    "        predictions.append(u)\n",
    "        phi = dictionary.transform(u)  # projection step\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_trajectory = predict(initial_test, K, rbf, num_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us compare the true and predicted trajectories, we see quite a good agreement!\n",
    "\n",
    "We have turned a 2D non-linear system into a 66-dimensional linear system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5), constrained_layout=True, sharex=True, sharey=True)\n",
    "\n",
    "for i, a in enumerate(ax):\n",
    "    a.plot(true_trajectory[:, 0, i], '-', label='True')\n",
    "    a.plot(predicted_trajectory[:, 0, i], '--', label='Predicted')\n",
    "    a.set_xlabel('$t$')\n",
    "    a.legend()\n",
    "\n",
    "ax[0].set_ylabel('$x$')\n",
    "ax[1].set_ylabel('$y$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(true_trajectory[:, 0, 0], true_trajectory[:, 0, 1], '-', label='True')\n",
    "plt.plot(predicted_trajectory[:, 0, 0], predicted_trajectory[:, 0, 1], '--', label='Predicted')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(-2, 2)\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic modes visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can look at the eigenvalues of $K$ (which are the same as $K^\\top$). What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues = np.linalg.eigvals(K)\n",
    "plt.scatter(eigenvalues.real, eigenvalues.imag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at eigenfunctions. Let us arrange them in descending orders of sizes of the absolute values of the eigenvalues (why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(K.T)\n",
    "sorted_indices = np.argsort(np.abs(eigenvalues))[::-1]\n",
    "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that each Koopman eigenfunction is approximated by\n",
    "$$\n",
    "    \\psi_j(x) = v_j^\\top \\varphi(x)\n",
    "$$\n",
    "where $v_j$ is the $j$-th eigenvector of $K^\\top$ and $\\varphi$ is the vector of dictionary functions.\n",
    "\n",
    "These functions are \"invariant\" under the action of the Koopman operator, i.e.\n",
    "$$\n",
    "    \\Kappa \\psi_j = \\lambda_j \\psi_j\n",
    "$$\n",
    "In other words, it keeps its shape under the evolution of the system, only changing in overall magnitude and phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at a few of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_grid_points = 100\n",
    "x = np.linspace(-2, 2, num_grid_points)\n",
    "y = np.linspace(-2, 2, num_grid_points)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "X = np.column_stack([X.ravel(), Y.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ef = 3  # number of eigenfucntions to plot\n",
    "\n",
    "fig, ax = plt.subplots(2, num_ef, figsize=(15, 5), constrained_layout=True)\n",
    "\n",
    "for i, a in enumerate(ax.T):\n",
    "    ef = rbf.transform(X) @ sorted_eigenvectors[:, i]\n",
    "    cf1 = a[0].contourf(x, y, ef.real.reshape(num_grid_points, num_grid_points))\n",
    "    cf2 = a[1].contourf(x, y, ef.imag.reshape(num_grid_points, num_grid_points))\n",
    "    fig.colorbar(cf1, ax=a[0])\n",
    "    fig.colorbar(cf2, ax=a[1])\n",
    "    a[0].set_title(f'Eigenfunction {i+1} (real)')\n",
    "    a[1].set_title(f'Eigenfunction {i+1} (imag)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We showed how to write a simple routine to perform EDMD (using the RBF basis) on a simple non-linear system to linearise it.\n",
    "\n",
    "Exercise:\n",
    "1. Choose other basis functions, and tweak other parameters\n",
    "2. Try this on other non-linear dynamics\n",
    "3. Can you try to reduce the dimensionality of the system from the dynamic mode decomposition?\n",
    "\n",
    "Note: while this notebook implements things from scratch, there are ready-made software libraries for DMD/Koopman operator based methods. For example,\n",
    "- https://pykoopman.readthedocs.io\n",
    "- https://github.com/PyDMD/PyDMD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsa5206_202324",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
