{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "sns.set(font_scale=1.5, style='darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Cleveland heart disease dataset\n",
    "\n",
    "**Context**\n",
    "\n",
    "This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to \n",
    "this date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4.\n",
    "\n",
    "**Content**\n",
    "\n",
    "\n",
    "*Attribute Information:*\n",
    "  * age \n",
    "  * sex \n",
    "  * chest pain type (4 values) \n",
    "  * resting blood pressure \n",
    "  * serum cholestoral in mg/dl \n",
    "  * fasting blood sugar > 120 mg/dl\n",
    "  * resting electrocardiographic results (values 0,1,2)\n",
    "  * maximum heart rate achieved \n",
    "  * exercise induced angina \n",
    "  * oldpeak = ST depression induced by exercise relative to rest \n",
    "  * the slope of the peak exercise ST segment \n",
    "  * number of major vessels (0-3) colored by flourosopy \n",
    "  * thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "\n",
    "**Acknowledgements**\n",
    "\n",
    "*Creators:*\n",
    "\n",
    "Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n",
    "University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n",
    "University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n",
    "V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.\n",
    "Donor: \n",
    "David W. Aha (aha '@' ics.uci.edu) (714) 856-8779"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the data using kaggle. You can also find the dataset in the UCI repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "kaggle.api.authenticate()\n",
    "\n",
    "kaggle.api.dataset_download_files(\n",
    "    'ronitf/heart-disease-uci',\n",
    "    path='./data',\n",
    "    quiet=False,\n",
    "    unzip=True,\n",
    "    force=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some columns containing multi-category data. In this case, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=['cp', 'restecg', 'slope', 'ca', 'thal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data and do a train-validation-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.pop('target').values\n",
    "x = data.values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, only fit the scaler on the training set so that no information about the test set enters the training process at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write some simple functions to build models, train models and evaluate models. This is so that we can avoid writing repeat code, as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(**layer_kwargs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu', input_shape=(27, ), **layer_kwargs))\n",
    "    model.add(Dense(128, activation='relu', **layer_kwargs))\n",
    "    model.add(Dense(1, activation='sigmoid', **layer_kwargs))\n",
    "    return model\n",
    "\n",
    "def train_and_save(model, path, force=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Looks for saved model in path, if found, load.\n",
    "    If not, compile, train and save model to path\n",
    "    If force=True, will always retrain\n",
    "    \"\"\"\n",
    "    model_save_dir = pathlib.Path(path)\n",
    "    model_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model_path = model_save_dir.joinpath('model.h5')\n",
    "    history_path = model_save_dir.joinpath('history.json')\n",
    "    \n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['binary_accuracy'],\n",
    "    )\n",
    "\n",
    "    if model_path.exists() and history_path.exists() and not force:\n",
    "        model.load_weights(str(model_path))\n",
    "        history = pd.read_json(history_path)\n",
    "    else:\n",
    "        batch_size = kwargs.get('batch_size', 128)\n",
    "        callbacks = kwargs.get('callbacks', [TqdmCallback(verbose=0)])\n",
    "        epochs = kwargs.get('epochs', 500)\n",
    "        validation_data = kwargs.get('validation_data', (x_test, y_test))\n",
    "        validation_split = kwargs.get('validation_split', 0)\n",
    "        history = model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=validation_data,\n",
    "            validation_split=validation_split,\n",
    "            verbose=0,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "        model.save_weights(str(model_path))\n",
    "        history = pd.DataFrame(history.history)\n",
    "        history.to_json(history_path)\n",
    "    return history\n",
    "\n",
    "def evaluate(model, train_data, test_data):\n",
    "    \"\"\"\n",
    "    Evaluate model on train/test sets\n",
    "    \"\"\"\n",
    "    eval_train = model.evaluate(*train_data, verbose=0)\n",
    "    eval_test = model.evaluate(*test_data, verbose=0)\n",
    "    print(f'Train - loss = {eval_train[0]:.3f}, acc = {eval_train[1]:.3f} ')\n",
    "    print(f'Test - loss = {eval_test[0]:.3f}, acc = {eval_test[1]:.3f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_and_save(model=model, path='baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model=model, train_data=(x_train, y_train), test_data=(x_test, y_test))\n",
    "history.plot(x=None, y=['loss', 'val_loss'])\n",
    "history.plot(x=None, y=['binary_accuracy', 'val_binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Norm Penalties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L^2$ Regularized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "model = build_model(kernel_regularizer=l2(alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_and_save(model=model, path='l2_regularized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model=model, train_data=(x_train, y_train), test_data=(x_test, y_test))\n",
    "history.plot(x=None, y=['loss', 'val_loss'])\n",
    "history.plot(x=None, y=['binary_accuracy', 'val_binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L^1$ Regularized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "model = build_model(kernel_regularizer=l1(alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_and_save(model=model, path='l1_regularized', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model=model, train_data=(x_train, y_train), test_data=(x_test, y_test))\n",
    "history.plot(x=None, y=['loss', 'val_loss'])\n",
    "history.plot(x=None, y=['binary_accuracy', 'val_binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kernel_dist(model, path):\n",
    "    _ = train_and_save(model=model, path=path, force=False)\n",
    "\n",
    "    kernel_abs_values = np.concatenate(\n",
    "        [\n",
    "            np.abs(w.numpy().ravel()) for w in model.weights if 'kernel' in w.name\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    ax = sns.histplot(kernel_abs_values, bins=50)\n",
    "    ax.set(yscale='log')\n",
    "    ax.set_xlabel('Weight Abs Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(path)\n",
    "    \n",
    "    return kernel_abs_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = plot_kernel_dist(model, 'l1_regularized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with the $L^2$ case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kernel_dist(model, 'l2_regularized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kernel_dist(model, 'baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will implement the early stopping method. This is supported in keras via the `EarlyStopping` callback. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we need to be careful about how to perform early stopping. First, we will define a validation set to use for early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_and_save(\n",
    "    model=model,\n",
    "    path='early_stopping',\n",
    "    callbacks=[TqdmCallback(verbose=0), early_stopper],\n",
    "    validation_data=None,\n",
    "    validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we retrain on the whole training set with the number of epochs equal to the stopping point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_and_save(\n",
    "    model=model,\n",
    "    path='early_stopping',\n",
    "    callbacks=[TqdmCallback(verbose=0)],\n",
    "    epochs=len(history),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model=model, train_data=(x_train, y_train), test_data=(x_test, y_test))\n",
    "history.plot(x=None, y=['loss', 'val_loss'])\n",
    "history.plot(x=None, y=['binary_accuracy', 'val_binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we consider adding noise to the inputs and features using the `GaussianNoise` layer, which adds i.i.d. Gaussian noise to each input/hidden features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GaussianNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(GaussianNoise(0.5, input_shape=(27, )))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(GaussianNoise(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(GaussianNoise(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_and_save(model=model, path='adding_noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model=model, train_data=(x_train, y_train), test_data=(x_test, y_test))\n",
    "history.plot(x=None, y=['loss', 'val_loss'])\n",
    "history.plot(x=None, y=['binary_accuracy', 'val_binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the efficient model-ensembling technique called dropout. This can be easily done using the pre-defined `Dropout` layer in `keras`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_shape=(27, )))\n",
    "model.add(Dropout(rate=0.95))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(rate=0.95))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use a rather large dropout rate because of data scarcity. Usually a rate of 0.4-0.6 are used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_and_save(model=model, path='dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model=model, train_data=(x_train, y_train), test_data=(x_test, y_test))\n",
    "history.plot(x=None, y=['loss', 'val_loss'])\n",
    "history.plot(x=None, y=['binary_accuracy', 'val_binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Combine the different techniques above to further improve performance.\n",
    "2. Explore how the hyper-parameters, such as regularization strengths, patience parameters or dropout rates affect the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "library.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
