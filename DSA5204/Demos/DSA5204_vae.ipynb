{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "\n",
    "In this notebook, we show how we can train variational autoencoders. Things to take note of that may be new\n",
    "  * using the `add_loss` method for fully custom loss functions\n",
    "  * writing noise generation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "sns.set(font_scale=1.5, style='darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the very familiar MNIST dataset to illustrate generative models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Samples using Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us use a simple fully connected autoencoder to attempt to sample the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a simple FC Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape\n",
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 2  # dimension of the latent space\n",
    "\n",
    "encoder = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(encoding_dim, activation='relu'),\n",
    "])\n",
    "\n",
    "decoder = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(encoding_dim, )),\n",
    "    Dense(784, activation='sigmoid'),\n",
    "    Reshape((28, 28)),\n",
    "])\n",
    "\n",
    "autoencoder = Sequential([encoder, decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'mnist_ae_gen.h5'\n",
    "try:\n",
    "    autoencoder.load_weights(filename)\n",
    "except:\n",
    "    autoencoder.fit(\n",
    "        x=x_train,\n",
    "        y=x_train,\n",
    "        batch_size=128,\n",
    "        epochs=50,\n",
    "        validation_data=(x_test, x_test),\n",
    "        callbacks=[TqdmCallback()],\n",
    "        verbose=0,\n",
    "    )\n",
    "    autoencoder.save_weights(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the reconstruction quality of the simple AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, n_plots=5):\n",
    "    \"\"\"Plot images of the digits\n",
    "    \"\"\"\n",
    "    n_plots = min(n_plots, len(images))\n",
    "    with sns.axes_style(\"dark\"):\n",
    "        fig, ax = plt.subplots(1, n_plots, figsize=(5*n_plots, 4))\n",
    "\n",
    "        for i, a in zip(images, ax):\n",
    "            a.imshow(i, cmap='Greys_r')\n",
    "            a.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_pred = autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(x_test)\n",
    "plot_images(x_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the Latent Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now explore the latent space, i.e.\n",
    "$$\n",
    "    z = \\mathrm{Encoder}(x)\n",
    "$$\n",
    "\n",
    "What we are going to do is the following:\n",
    "  * Given two sample images $x^{(1)}$ and $x^{(2)}$, we obtain their latent states\n",
    "  $$\n",
    "      z^{(i)} = \\mathrm{Encoder}(x^{(i)})\n",
    "  $$\n",
    "  * Consider their convex combination in latent space\n",
    "  $$\n",
    "      z(r) = (1-r) z^{(1)} + r z^{(2)}\n",
    "  $$\n",
    "  where $r\\in [0,1]$. As $r$ varies, this interpolates between the two latent representations\n",
    "  * We then explore the decoded image\n",
    "  $$\n",
    "      x'(r) = \\mathrm{Decoder}(z(r))\n",
    "  $$\n",
    "  as $r$ varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_1 = x_test[0]\n",
    "image_2 = x_test[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the two images we picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images([image_1, image_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute and plot the interpolation through the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_1 = encoder.predict(image_1[None, :, :])\n",
    "z_2 = encoder.predict(image_2[None, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_images = [image_1]\n",
    "for r in np.linspace(0, 1, 10):\n",
    "    z = z_2 * r + z_1 * (1-r)\n",
    "    interpolated_images.append(np.squeeze(decoder.predict(z)))\n",
    "interpolated_images.append(image_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(interpolated_images, n_plots=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen, the images are not varying continuously. To see this more clearly, we can plot the entire latent space and the images that are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def plot_latents(models, data, n=15):\n",
    "    \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector\n",
    "    \"\"\"\n",
    "    encoder, decoder = models\n",
    "    x_test, y_test = data\n",
    "\n",
    "    z_mean = encoder.predict(x_test)\n",
    "    if type(z_mean) == list:  # For compatibility later\n",
    "        z_mean = z_mean[0]\n",
    "    \n",
    "    with sns.axes_style(\"dark\"):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "        im = ax1.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
    "        plt.colorbar(im, ax=ax1)\n",
    "        ax1.set_xlabel(\"$z_0$\")\n",
    "        ax1.set_ylabel(\"$z_1$\")\n",
    "        \n",
    "        digit_size = 28\n",
    "        figure = np.zeros((digit_size * n, digit_size * n))\n",
    "        x_min, x_max = z_mean[:, 0].min(), z_mean[:, 0].max()\n",
    "        y_min, y_max = z_mean[:, 1].min(), z_mean[:, 1].max()\n",
    "        grid_x = np.linspace(x_min, x_max, n)\n",
    "        grid_y = np.linspace(y_min, y_max, n)[::-1]\n",
    "\n",
    "        for i, yi in enumerate(grid_y):\n",
    "            for j, xi in enumerate(grid_x):\n",
    "                z_sample = np.array([[xi, yi]])\n",
    "                x_decoded = decoder.predict(z_sample)\n",
    "                digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "                figure[i * digit_size:(i + 1) * digit_size,\n",
    "                       j * digit_size:(j + 1) * digit_size] = digit\n",
    "\n",
    "        start_range = digit_size // 2\n",
    "        end_range = (n - 1) * digit_size + start_range + 1\n",
    "        pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "        sample_range_x = np.round(grid_x, 1)\n",
    "        sample_range_y = np.round(grid_y, 1)\n",
    "        ax2.set_xlabel(\"$z_0$\")\n",
    "        ax2.set_ylabel(\"$z_1$\")\n",
    "        im = ax2.imshow(figure, cmap='Greys_r')\n",
    "        ax2.set_xticklabels([])\n",
    "        ax2.set_yticklabels([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latents(\n",
    "    models=(encoder, decoder),\n",
    "    data=(x_test, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the features are not well disentangled, in particular, the transition between numbers is very non-smooth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now apply the VAE ideas developed in class to generating handwritten numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a VAE Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder architecture\n",
    "\n",
    "Let's first build the encoder network. Unlike the usual AE, for the encoder we want 3 outputs\n",
    "  * $y_1$ (mean of the latent Gaussian distribution)\n",
    "  * $y_2$ (log(std) of the latent Gaussian distribution)\n",
    "  * $z$ (sample of the random Gaussian with mean and std from above\n",
    "\n",
    "We can build $y_1,y_2$ easily by the outputs of two different `Dense` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(28, 28))\n",
    "h = Flatten()(x)\n",
    "h = Dense(128, activation='relu')(h)\n",
    "y1 = Dense(encoding_dim)(h)\n",
    "y2 = Dense(encoding_dim)(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to build a custom layer in order to obtain $z$ via a sampling procedure, using `tf.random.normal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gaussian(mean_and_logstd):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "    \"\"\"\n",
    "    mean, logstd = mean_and_logstd\n",
    "    u = tf.random.normal(tf.shape(mean))\n",
    "    return mean + tf.math.exp(logstd) * u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sample = Lambda(sample_gaussian)([y1, y2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build the encoder model, noting that there are a total of 3 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(x, [y1, y2, z_sample], name='encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder model is the factorized Bernoulli model, thus we only need to build a network that outputs a value between [0,1] ($s$) per pixel of the output. This can be done using the sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_in = Input(shape=(encoding_dim, ))\n",
    "h = Dense(128, activation='relu')(z_in)\n",
    "h = Dense(784, activation='sigmoid')(h)\n",
    "s = Reshape((28, 28))(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Model(z_in, s, name='decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining into a VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combine into a VAE, we simply take the generated latent state $z$ from the encoder network and feed it into the decoder network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = decoder(encoder(x)[-1])\n",
    "vae = Model(x, xp, name='vae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the ELBO Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As derived in class, the (negative) ELBO loss consisting of two parts:\n",
    "  * Reconstruction loss\n",
    "  $$\n",
    "      \\mathrm{BinaryCrossEntropy}(\\theta,\\phi)\n",
    "  $$\n",
    "  * KL-divergence loss\n",
    "  $$\n",
    "      \\mathrm{KL(\\theta,\\phi)} \n",
    "      =\n",
    "      \\underbrace{\n",
    "          \\frac{1}{2} \\| y_1 \\|^2      \n",
    "      }_{\\mathrm{KL}_1}\n",
    "      +\n",
    "      \\underbrace{\n",
    "          \\frac{1}{2} \\| e^{y_2} \\|^2\n",
    "      }_{\\mathrm{KL}_2}\n",
    "      -\n",
    "      \\underbrace{\n",
    "          \\sum_j y_{2,j}\n",
    "      }_{\\mathrm{KL}_3}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss = binary_crossentropy(\n",
    "    tf.reshape(x, [-1, 784]),\n",
    "    tf.reshape(xp, [-1, 784]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_loss_1 = 0.5 * tf.reduce_sum(y1**2, axis=-1)\n",
    "kl_loss_2 = 0.5 * tf.reduce_sum(tf.math.exp(y2)**2, axis=-1)\n",
    "kl_loss_3 = - tf.reduce_sum(y2, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_loss = kl_loss_1 + kl_loss_2 + kl_loss_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the losses together. Note that the BCE scales by the output dimension (784) by default, so to get the right scaling we will scale all of the KL loss by the same amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_loss = reconstruction_loss + (1.0 / 784) * kl_loss\n",
    "vae_loss = tf.reduce_mean(vae_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add the loss to the model via\n",
    "```python\n",
    "    vae.add_loss(vae_loss)\n",
    "```\n",
    "This is the most general way to use custom loss functions. However, I suggest to use as much as possible the API for `tensorflow.keras.losses` and subclassing from there, if possible. For the case of VAE, the current method is the most convenient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.add_loss(vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_filename = 'mnist_vae_encoder.h5'\n",
    "dec_filename = 'mnist_vae_decoder.h5'\n",
    "try:\n",
    "    encoder.load_weights(enc_filename)\n",
    "    decoder.load_weights(dec_filename)\n",
    "except:\n",
    "    vae.fit(\n",
    "        x=x_train,\n",
    "        y=None,\n",
    "        epochs=50,\n",
    "        batch_size=128,\n",
    "        validation_data=(x_test, None),\n",
    "        callbacks=[TqdmCallback()],\n",
    "        verbose=0,\n",
    "    )\n",
    "    encoder.save_weights('mnist_vae_encoder.h5')\n",
    "    decoder.save_weights('mnist_vae_decoder.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Latent Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us explore the latent space trained by VAE and try interpolation and generation of new samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation\n",
    "\n",
    "We start with the same interpolation idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, z_1 = encoder.predict(image_1[None, :, :])\n",
    "_, _, z_2 = encoder.predict(image_2[None, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_images_vae = [image_1]\n",
    "for r in np.linspace(0, 1, 10):\n",
    "    z = z_2 * r + z_1 * (1-r)\n",
    "    interpolated_images_vae.append(np.squeeze(decoder.predict(z)))\n",
    "interpolated_images_vae.append(image_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(interpolated_images, n_plots=12)\n",
    "plot_images(interpolated_images_vae, n_plots=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Distribution and Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latents(\n",
    "    models=(encoder, decoder),\n",
    "    data=(x_test, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "1. Try VAEs on other image tasks, e.g. generation and interpolation between human faces\n",
    "2. Try convolution layers in VAEs\n",
    "3. Try using non-diagonal latent distribution model (you need to derive the corresponding loss functions!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
